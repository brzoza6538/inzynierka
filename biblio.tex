\begin{itemize}
  \item \url{https://optimization.cbe.cornell.edu/index.php?title=AdaGrad}
  \item \url{https://en.wikipedia.org/wiki/Stochastic_gradient_descent#AdaGrad}
  \item \url{https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html}
  \item \url{https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam}
  \item \url{https://www.geeksforgeeks.org/deep-learning/adam-optimizer/}
  \item \url{https://en.wikipedia.org/wiki/Limited-memory_BFGS}
  \item \url{https://dl.acm.org/doi/pdf/10.1145/279232.279236}
  \item \url{https://esezam.okno.pw.edu.pl/mod/book/view.php?id=1245&chapterid=1950}
  \item \url{https://www.researchgate.net/publication/353782316_NL-SHADE-RSP_Algorithm_with_Adaptive_Archive_and_Selective_Pressure_for_CEC_2021_Numerical_Optimization/link/6226e6573c53d31ba4b03f4a/download?_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6InB1YmxpY2F0aW9uIiwicGFnZSI6InB1YmxpY2F0aW9uIn19}
\end{itemize}
